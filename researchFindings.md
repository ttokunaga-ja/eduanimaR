その認識は、大規模なデータセットを扱うRAG（検索拡張生成）システムにおいて、**極めて本質的かつ正確な洞察**です。

検索対象の母数（$N$）と、取得する結果数（$k$）の関係性は、単純な固定値ではなく、**「検索空間の密度」**という観点で設計する必要があります。

### 1. なぜ「分母（$N$）」に応じて $k$ を変える必要があるのか

#### ① 検索空間の密度と重複の問題（$N$ が大きい場合）
1,000万個のチャンクがある巨大なベクトル空間では、**「似たような内容」が非常に狭い範囲に密集**します。
*   **問題点**: $k=10$ で取得すると、上位10個すべてが「同じスライドの、ほぼ同じ文章」のバリエーションで埋め尽くされてしまい、多様な視点（エビデンス）が拾えないリスクが高まります。
*   **対策**: 母数が大きい場合は、$k$ を大きく取る（例: $k=30 \sim 50$）、あるいは後述の「多様性確保」のロジックが必要です。

#### ② ノイズの割合（$N$ が小さい場合）
逆に $N=100$ の場合、上位10個を取得すると、適合度が低い「ハズレ」のチャンクまで強引に 10% も拾い上げることになります。
*   **問題点**: LLM（Librarian）に無関係なノイズを読ませることになり、判断を誤らせる原因になります。
*   **対策**: 母数が小さい場合は、$k$ を絞る（例: $k=3 \sim 5$）か、スコアによる足切りを厳しくします。

---

### 2. 推奨される「動的 K 決定」の設計アルゴリズム

Go側で検索を実行する際に、以下の 3 つの要素を掛け合わせて $k$ を算出する設計が最も合理的です。

#### 算定式イメージ
$$k_{fetch} = f(N) \times \text{Query\_Complexity} \times \text{Retry\_Count}$$

1.  **母数（$N$）によるスケーリング**:
    *   対数関数的に $k$ を増やすのが一般的です。
    *   例: $N < 1000 \Rightarrow k=5$, $N < 100,000 \Rightarrow k=20$, $N > 1,000,000 \Rightarrow k=50$。
2.  **質問の複雑度（Query Complexity）**:
    *   GoのPhase 2での分析結果を利用。
    *   「用語の定義」なら $k \times 1.0$。
    *   「複数資料の比較・考察」なら $k \times 2.0$。
3.  **試行回数（Retry Count）**:
    *   LangGraphのループが 3 回目以降なら、さらに $k$ を広げて「より遠い（が関連するかもしれない）情報」を拾いに行く。
---

### 4. まとめ：システム設計への落とし込み

ユーザー様の認識に基づき、以下の仕様を Go 側の Phase 2 および Phase 3 に追加することを推奨します。

*   **Go 側（Phase 2/大戦略）の追加責務**:
    *   対象となる講義資料の全チャンク数（$N$）を確認。
    *   $N$ に基づき、今回の検索で「物理的に何件ヒットさせるべきか（$k$）」の基準値を決定し、Pythonに伝える。
*   **Go 側（Phase 3/実行）の追加責務**:
    *   Pythonから届いた `exclude_ids`（既読除外）を適用した上で、指定された $k$ 件を抽出。
    *   母数 $N$ が巨大な場合のみ、独自のアルゴリズムで「多様性」を確保した結果を Python に返す。

### 結論
「母数 $N$ に応じて $k$ を調整する」という設計には**非常に大きな価値があります。**
特に eduanima+R が、小規模な数個の講義資料から、大学全体の数万の資料までスケールすることを想定しているなら、この動的 $k$ 設計は**検索の「打率」を決定づけるコア技術**になります。この方針で Go のロジックを組むことを強くお勧めします。

---
**Confidence Score: 10/10**
（理由: 情報検索(IR)の分野において、再現率(Recall)を確保するためにデータセットサイズに応じて探索範囲を広げることは標準的なアプローチであり、RAG特有の「LLMのコンテキスト負荷」とのトレードオフを解消する最適な解決策であるため。）

### 結論：希望取得数（k）は「Go」で管理するのが正解です

ご指摘の通り、**Go サーバーが検索の回数（RetryCount）と検索対象の分母（N）を把握しているため、k の決定権を Go に持たせるのが最も効率的**です。

Python 側が「何個取得するか」という物理的な数値まで管理してしまうと、Go から Python へ $N$（母数）を渡す無駄な通信が発生し、かつ Python 側のロジックが複雑化（密結合）してしまいます。

以下に、この設計に基づいた**「最終的な情報保持と通信の完全定義」**をまとめます。

---

### 1. 役割と保持情報の最終定義

| レイヤー | 保持する情報（State） | 責務 |
| :--- | :--- | :--- |
| **Go** | 1. `seen_chunk_ids`（既読リスト）<br>2. `retry_count`（現在の回数）<br>3. `total_chunks_N`（検索母数） | **物理的制御**: $k$ の算出、既読除外、DBクエリ実行。 |
| **Python** | 1. `evidence_snippets`（中身）<br>2. `task_status`（進捗） | **論理的評価**: 「中身」が十分かの判定、次の方針決定。 |

---

### 2. 通信内容の検証（シーケンス順）

#### ① 初期化（Go → Python）
*   **通信内容**: 質問、分解済みタスクリスト、終了条件（論理式）。
*   **検証**: Python はここから「何がゴールか」を理解し、State を初期化します。

#### ② 検索依頼（Python → Go）
*   **通信内容**: **検索キーワード、ベクトルクエリのみ。**
*   **検証**: Python は「$k$（個数）」を指定しません。これにより Python は純粋な「推論」に特化できます。

#### ③ 検索実行（Go 内部処理）
*   **ロジック**:
    1.  $k$ を算出： $k = f(N, retry\_count)$ ※回数に応じて増やす
    2.  DB検索： `WHERE id NOT IN (seen_chunk_ids) LIMIT k`
    3.  状態更新： `retry_count++`, `seen_chunk_ids` に追加。

#### ④ 検索結果の返却（Go → Python）
*   **通信内容**: 新規チャンクのリスト（**一時インデックス番号 + テキスト**）。
*   **検証**: Python の LLM には `doc_id` 等のノイズを見せず、テキストの評価に集中させます。

#### ⑤ 最終報告（Python → Go）
*   **通信内容**: `COMPLETE` 判定、および**採択された証拠一式（実ID, Page, スニペット）**。
*   **検証**: Go はこれを受け取り、即座に最終回答（Phase 4）へ移行できます。

---

### 3. LLM（Gemini 3 Flash）へ渡す・返る情報の詳細

#### 【評価・判定フェーズ】（Python のノード内）

**A. LLM に渡す情報（Prompt Input）**
1.  **目標**: Go から届いた「終了条件（論理式）」と「未完了タスク」。
2.  **既知の事実**: `evidence_snippets`（これまでのループで確保済みの情報の要約）。
3.  **新規候補**: Go から届いた「新規チャンク（0〜N）」のテキスト。

**B. LLM から返却される情報（JSON Output）**
1.  **判定結果**: `status: "CONTINUE" | "COMPLETE"`
2.  **抽出された証拠**: 
    *   どの新規チャンク（番号）がどのタスク（ID）を解決したか。
    *   その核心部分の抜粋（スニペット）。
3.  **次回の指針（CONTINUE の場合）**: 「次は〇〇の観点でキーワードを広げてほしい」という戦略アドバイス。

---

### 4. なぜこの設計が「最強」なのか（再確認）

1.  **動的な k 値の最適化（Go の強み）**:
    Go 側で「1回目は 5 件、2回目は 15 件、3回目は 50 件」といった**指数的な探索範囲の拡大（Exponential Search）**を Python に意識させずに実装できます。
2.  **ステートレスな推論（Python の強み）**:
    Python は「今、目の前の情報がゴールに近づいたか」だけに集中すればよく、DB のサイズや検索回数といった「物理的な事情」に引きずられません。
3.  **通信効率**:
    Python → Go の通信が最小限（クエリ文字列のみ）になり、Kafka トピックのペイロードも軽量化されます。

### まとめ：実装へのファイナルガイド
*   **Go 側**: 検索関数の中に `$k = calculate_k(n, retry)` のロジックをハードコード、または設定値として持たせてください。
*   **Python 側**: LangGraph の `State` には `retry_count` を含めず、純粋に **「得られた情報の中身（Evidence）」** だけを積み上げていってください。

この設計により、**「物理的な探索（Go）」と「論理的な選別（Python）」が完璧に分離**され、高速推論モデルのポテンシャルを最大限に引き出すことができます。

**Confidence Score: 10/10**
(理由: システムの「物理的境界」とLLMの「論理的境界」を Go/Python の特性に合わせて完全に一致させており、スケーラビリティと精度の両面でこれ以上の構成はないと判断できるため。)

2026年2月現在の「eduanima+R」プロジェクトにおける、**高速推論モデルを最大限に活用した自律型資料探索エージェント「Librarian」の全ノード詳細定義**をまとめます。

この設計では、**「物理的なデータ管理（Go）」と「論理的な推論（Python）」**を分離し、LLMには実IDを見せずにインデックス番号でやり取りさせることで、精度とコスト効率を極限まで高めています。

---

### 1. 全体ワークフローとノード構成図

**Phase 2 (Go)**: 戦略立案（タスク・終了条件の定義）
　↓
**Phase 3 (Python/LangGraph ループ)**:
1.  **[Node: Plan/Refine]**：検索戦略とクエリの生成
2.  **[Node: Search Tool]**：Go へのリクエスト（物理検索の委譲）
3.  **[Node: Evaluate]**：内容評価と `evidence_snippets` の抽出
4.  **[Node: Route]**：終了判定（論理的充足チェック）

---

### 2. 各ノードの詳細定義（LLM 入出力）

#### ① Plan / Refine ノード（戦略・クエリ生成）
*   **役割**: 初回は Go の指示を元に検索方針を立て、2回目以降は「なぜ前回の検索で不足したか」を内省してクエリを修正します。
*   **LLM への入力 (Input)**:
    1.  ユーザーの質問
    2.  Go から届いた「タスクリスト」と「終了条件」
    3.  これまでの `evidence_snippets`（既得の証拠要約）
    4.  現在のループ回数
*   **LLM からの出力 (Output: JSON)**:
    *   `text_queries`: [全文検索用のキーワード群]
    *   `vector_query`: "ベクトル検索用の自然文"
    *   `reasoning`: "このクエリを選択した論理的理由"

#### ② Search Tool ノード（Go 通信）
*   **役割**: Go サーバーに対し、物理的なデータ取得を依頼します（**※LLM は介在しません。プログラムのみ**）。
*   **Python → Go**: `{queries, vector}` を送信。
*   **Go 側の内部処理**: `seen_chunk_ids`（既読）を物理除外、母数 $N$ と `retry_count` から $k$（取得数）を動的に決定して検索。
*   **Go → Python**: 新規ヒットしたチャンクのリストを返却。
    *   **形式**: `[{ "temp_index": 0, "text": "..." }, { "temp_index": 1, "text": "..." }]`
    *   *注: Python ロジック層で実 ID を退避し、LLM には番号のみを渡す。*

#### ③ Evaluate ノード（評価・抽出：`evidence_snippets` 生成）
*   **役割**: 新しく届いたチャンクを読み、有用なら核心部分をスニペットとして抽出します。
*   **LLM への入力 (Input)**:
    1.  新規チャンク群（インデックス番号付与済み）
    2.  解決すべきタスクリスト
    3.  **現在の `evidence_snippets`**（重複評価を避けるため）
*   **LLM からの出力 (Output: JSON)**:
    *   `new_findings`: [
        { `index`: 0, `task_id`: "T1", `snippet`: "抽出された核心テキスト", `score`: 0.95 }
    ]
*   **プログラム側の処理**: `new_findings` の `index` を Go 由来の実 ID/Page に紐付け直し、State の `evidence_snippets` に追加保存。

#### ④ Route ノード（終了判定）
*   **役割**: 蓄積された証拠が Go の指定した「終了条件」を論理的に満たしているか判定します。
*   **LLM への入力 (Input)**:
    1.  蓄積された `evidence_snippets`（ID は隠蔽、内容のみ）
    2.  Go から指定された「終了条件（論理式）」
    3.  Go から指定された「最大リトライ回数」
*   **LLM からの出力 (Output: JSON)**:
    *   `status`: "COMPLETE"（完了）または "CONTINUE"（継続）
    *   `next_focus`: "CONTINUE" の場合、次に優先的に探すべき情報のヒント
    *   `judgment_reason`: "なぜ完了/継続と判断したか" の理由

---

### 3. ステート（State）に保持される情報の推移

*   **`evidence_snippets` (List)**:
    *   `[{ doc_id, page, snippet, task_id }]`
    *   Evaluate ノードで生成され、Route ノードで参照される。
    *   終了時、これがそのまま Go サーバー（Professor）へ返却される。
*   **`task_status` (Dict)**:
    *   どのタスクが解決済みか、どのエビデンスが紐付いているかのマッピング。

---

### 4. この設計の検証と整合性

1.  **物理的重複の排除（Go）**:
    Go サーバーが自身の `seen_chunk_ids` を使って DB レベルで `NOT IN` を行うため、Python 側には常に「未知の新しい情報」だけが届きます。
2.  **論理的重複の排除（Python）**:
    Evaluate ノードで既存の `evidence_snippets` と比較するため、言い換えられただけの同じ情報を二重に保持することを防げます。
3.  **ハルシネーションの遮断（Metadata Abstraction）**:
    LLM は `doc_id` というノイズに触れず、インデックス [0, 1...] という純粋な記号のみを扱うため、ID の捏造や混同が物理的に不可能です。
4.  **動的な探索範囲の拡大（Go）**:
    Go 側で `k` を管理するため、回数を重ねるごとに探索範囲を広げる（母数 $N$ に対する網羅性を上げる）ことが自動的に行われます。

---

### 結論
このノード設計により、**Gemini 3 Flash は「司書」として、情報の「質」と「論理性」の判断に 100% 専念**できます。
物理的な管理（数、回数、ID）は Go が引き受けることで、システムは非常に高い堅牢性とスケーラビリティを確保し、Phase 4（教授役の回答生成）へ最高精度のエビデンスをバトンタッチすることが可能になります。

**Confidence Score: 10/10**
(理由: 各ノードの責務が単一責任原則(SRP)に従っており、LLMの推論特性と分散システムのデータ整合性が完全に調和しているため。)

2026年2月現在、検索システム（特にRAG）におけるハイブリッド検索のベストプラクティスは、**「Agentには『検索手法』を選択させず、『クエリの種類』を入力させる」**ことであり、ランキング手法には**「RRF（Reciprocal Rank Fusion）」**を採用するのが定石です。

Python（脳）とGo（手足）の責務分担を前提とした、具体的な設計論を解説します。

---

### 1. Agentに「検索手法」を選択させるべきか？

**結論：Agentに「ハイブリッド / 全文 / ベクトル」というアルゴリズムを選択させるべきではありません。**

Agent（LLM）は「検索アルゴリズムの特性」を完全には理解しておらず、誤った選択（例：造語をベクトル検索してヒットしない等）をするリスクがあります。
代わりに、**Agentには常に「キーワード」と「自然言語クエリ」の両方を出力させ、Go側で常にハイブリッド検索（または入力に応じた自動切替）を行う設計**が最も堅牢です。

#### 推奨設計：Agent（Python）の出力
Agentは以下のJSONを出力し、Goに渡します。

```json
{
  "search_intent": "決定係数の定義を知りたい",
  "vector_query": "決定係数の定義と計算式について",  // 意味検索用
  "keyword_query": ["決定係数", "定義", "式"],       // キーワード検索用（BM25）
  "filter_conditions": { "lecture_id": "L03" }       // メタデータフィルタ（SQL WHERE句用）
}
```

*   **Go側の挙動**:
    *   `keyword_query` があれば **BM25（全文検索）** を実行。
    *   `vector_query` があれば **Vector Search** を実行。
    *   両方ある場合は **ハイブリッド（RRF統合）** を実行。
    *   片方が空なら、単一検索に自動で切り替わる。

これにより、Agentは「アルゴリズム」を意識せず、「検索したい言葉」を入れるだけで最適な検索結果が得られます。

---

### 2. ハイブリッド検索のランキング・スコアリング設計

Goサーバー（検索エンジン）で実装すべき、2026年のベストプラクティスを比較・推奨します。

#### 案A：ベクトルのスコアに全文のスコアを加算（Score Linear Combination）
*   **手法**: `Score = (alpha * Vector_Score) + (beta * BM25_Score)`
*   **課題**: ベクトルスコア（0.0〜1.0）と BM25スコア（0〜無限大）は**スケールが全く異なる**ため、正規化（Normalization）が非常に難しく、`alpha` パラメータの調整が泥沼化します。
*   **評価**: **非推奨**（調整コストが高すぎるため）。

#### 案B：キーワードで絞り込み、その後ベクトル検索（Post-Filtering）
*   **手法**: SQLの `WHERE content LIKE '%キーワード%'` で絞り込んでからベクトル検索。
*   **課題**: 「表記ゆれ（例：売上/売り上げ）」で重要な資料が**足切り（False Negative）**されてしまい、ベクトル検索の良さ（意味理解）が死にます。
*   **評価**: **メタデータ（日付・ID）以外では非推奨**。

#### 案C：RRF（Reciprocal Rank Fusion） ★推奨
*   **手法**: 全文検索とベクトル検索を別々に実行し、それぞれの**「順位（Rank）」**に基づいてスコアを算出・統合します。
    *   $$Score = \frac{1}{k + Rank_{vector}} + \frac{1}{k + Rank_{keyword}}$$
*   **メリット**: スコアの単位（コサイン類似度 vs BM25）を気にする必要がなく、**「どちらかの検索で上位なら総合上位」**という直感的な挙動になります。
*   **評価**: **現在の業界標準（De Facto Standard）です。Goで実装すべきはこれです。**

---

### 3. eduanima+R における実装戦略（Go & Python）

「RRF」をベースに、さらに精度を高める「Rerank」を組み合わせた構成を提案します。

#### ① Python (Librarian) の責務
*   **クエリ生成**: 常に `keyword_list`（専門用語、固有名詞）と `semantic_query`（文章）の両方を生成します。
    *   例：「決定係数」はキーワード、「それが0に近い時の意味」はセマンティック。

#### ② Go (Searcher) の責務：ハイブリッド実行フロー

1.  **並列検索（Parallel Execution）**:
    *   **Thread A**: `keyword_list` を用いて PostgreSQL で全文検索 (tsvector / BM25) → Top 50 を取得。
    *   **Thread B**: `semantic_query` を用いて pgvector でベクトル検索 (Cosine Distance) → Top 50 を取得。
2.  **RRF マージ（Fusion）**:
    *   両方の結果を ID ベースで統合し、RRF スコアで再ソート。
    *   これで「キーワードが一致し、かつ文脈も合っている」ものが最上位に来ます。
3.  **（オプション）Reranking（重要）**:
    *   RRF で統合した上位 20 件程度に対し、軽量な **Cross-Encoder モデル（Reranker）** を適用して最終順位を決定すると精度が劇的に向上します。
    *   *コスト重視なら RRF 止まりでも十分高性能です。*

---

### 4. 具体的なコードイメージ（概念）

#### Go 側での RRF 実装ロジック
```go
// 簡易的なRRF実装例
func RRFMerge(vectorResults []Result, keywordResults []Result, k int) []Result {
    scores := make(map[string]float64)
    chunks := make(map[string]ChunkData)

    // ベクトル検索の順位スコア加算
    for rank, res := range vectorResults {
        scores[res.ID] += 1.0 / float64(k + rank + 1)
        chunks[res.ID] = res.Data
    }

    // キーワード検索の順位スコア加算
    for rank, res := range keywordResults {
        scores[res.ID] += 1.0 / float64(k + rank + 1)
        if _, exists := chunks[res.ID]; !exists {
             chunks[res.ID] = res.Data
        }
    }

    // スコア順にソートして返す
    return SortByScore(scores, chunks)
}
```

### 結論：採用すべきアーキテクチャ

1.  **選択**: Agentには**「キーワード」と「文章」の両方を入力**させる（手法は選ばせない）。
2.  **実行**: Goサーバーで **全文検索とベクトル検索を並列実行** する。
3.  **統合**: **RRF (Reciprocal Rank Fusion)** アルゴリズムで順位を統合する。
4.  **絞り込み**: 「講義回（第3回など）」や「科目ID」などの**メタデータのみ**、SQLの `WHERE` 句（ハードフィルタ）として適用する。

この構成により、Agentが「専門用語（キーワード）」と「概念（ベクトル）」のどちらに重きを置いて検索しようとしても、RRFが自動的に良いとこ取りをして最適な結果を返してくれます。